# Prompting-techniques

## Step-Back Prompting

**Resources**
- [ ] [TAKE A STEP BACK: EVOKING REASONING VIA AB-
STRACTION IN LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2310.06117)

## COT Prompting

思想链（Chain-of-Thought, CoT）是一种旨在提升大型语言模型（LLM）在复杂推理任务中表现的 Prompting 技术。其核心思想是引导模型在给出最终答案之前，先输出一系列连贯的、中间的推理步骤。这模仿了人类解决问题时“一步一步来”的思考过程。

### 为什么 CoT 有效？

1.  **分解任务**: 将一个复杂问题分解成多个更小、更易于管理的部分。
2.  **减少错误**: 模型可以逐步检查和修正自己的推理过程，减少在计算或逻辑链条中过早出错的概率。
3.  **可解释性**: 推理过程是透明的，用户可以清楚地看到模型是如何得出结论的，便于发现和纠正错误。

### 示例

#### 示例 1: 标准 Prompting (Standard Prompting)

**Prompt:**
```
问：自助餐厅有23个苹果。如果他们用了20个来做午餐，然后又买了6个，他们现在有多少个苹果？

答：
```

**模型可能的回答 (可能会出错):**
```
29个
```
*(这个答案是错误的，模型可能只是简单地将数字相加 23 + 6 = 29)*

#### 示例 2: CoT Prompting

**Prompt:**
```
问：自助餐厅有23个苹果。如果他们用了20个来做午餐，然后又买了6个，他们现在有多少个苹果？

请一步一步思考，最后给出答案。
```

**模型理想的回答:**
```
好的，我们来一步步分析：
1. 自助餐厅一开始有 23 个苹果。
2. 他们用了 20 个做午餐，所以剩下 23 - 20 = 3 个苹果。
3. 然后他们又买了 6 个苹果，所以现在的苹果总数是 3 + 6 = 9 个。

所以，他们现在有9个苹果。
```

通过引导模型进行逐步思考，我们得到了正确的答案，并且整个逻辑过程清晰可见。

## Self-Consistency Prompting

Self-Consistency 是一种在 **思想链（CoT）** 基础上构建的更强大的 Prompting 策略，它通过生成多个不同的推理路径并对最终结果进行“投票”，来选择最一致的答案，从而显著提高复杂推理任务的可靠性。

这种策略通常需要通过编程调用 API 来实现，因为它依赖于对模型生成过程的精确控制。

### 实现步骤

1.  **使用 CoT Prompt**: 构造一个引导模型进行逐步思考的 Prompt。
2.  **多次生成 (Sampling)**: 使用同一个 CoT Prompt 多次调用 LLM API。关键一步是将 API 的 `temperature` 参数设置为一个大于 0 的值（如 0.5），以鼓励模型产生多样化的输出。
3.  **收集多个推理路径**: 保存每一次 API 调用返回的、包含完整推理过程的答案。
4.  **投票选出最终答案**: 编写代码解析所有输出，提取每个推理路径的最终结论，并选择出现次数最多的那个作为最终答案。

### 示例

**问题**: "如果 5 台机器 5 分钟能生产 5 个零件，那么 100 台机器生产 100 个零件需要多长时间？"

通过多次调用（`temperature > 0`），我们可能得到三个不同的推理路径：

-   **路径 1**: "一台机器 5 分钟生产 1 个零件。所以 100 台机器 5 分钟能生产 100 个零件。答案是 **5 分钟**。"
-   **路径 2**: "5 台机器 5 分钟生产 5 个零件，意味着每台机器生产一个零件需要 5 分钟。因此，100 台机器生产 100 个零件，每台机器还是各自生产一个，所以也需要 **5 分钟**。"
-   **路径 3**: "5 台机器是 5 分钟，所以 100 台机器是 (100/5) * 5 = 100 分钟。答案是 100 分钟。" (这是一个错误的推理)

**投票结果**:
*   "5 分钟": 2票
*   "100 分钟": 1票

最终，我们选择 "5 分钟" 作为最可靠的答案。

## Tree of Thoughts (ToT) Prompting

Tree of Thoughts (ToT) 是一个超越了传统 Prompting 技巧的高级问题求解框架。它将模型的思考过程从线性的“思想链”扩展为一棵“思想树”，让模型可以主动探索、评估和选择不同的推理路径。

ToT 框架**必须**通过编程和 API 调用来实现，因为它需要一个外部程序来协调 LLM 完成复杂的生成、评估和搜索循环。

### 与 CoT 和 Self-Consistency 的区别

-   **CoT**: 生成一条线性的思考路径 (`A -> B -> C`)。
-   **Self-Consistency**: 生成多条独立的线性路径，然后对最终结果进行投票。
-   **ToT**: 在每一步都探索多个分支，并**主动评估**每个分支的“好坏”，然后智能地决定下一步要探索哪个分支，或者放弃哪个分支。

### ToT 框架的工作流程

实现 ToT 需要编写一个程序来协调 LLM 完成以下四个核心任务：

1.  **分解问题 (Decomposition)**: 将问题分解成一系列可以逐步解决的步骤或“想法”。
2.  **生成想法 (Thought Generation)**: 在树的每个节点（代表当前状态），调用 LLM API 生成多个可能的下一步想法。
    *   *Prompt 示例*: `"对于当前的状态，请提出3个可能的、有前景的下一步解决方案。"`
3.  **评估状态 (State Evaluation)**: 这是 ToT 的精髓。再次调用 LLM API（使用一个专门的评估 Prompt）来评估每个新生成的想法的价值或可行性。
    *   *Prompt 示例*: `"对于当前方案[方案描述]，评估它最终能成功解决问题的可能性，并给出一个从1到10的分数。"`
4.  **搜索算法 (Search Algorithm)**: 您的外部程序需要实现一个搜索算法（如广度优先搜索 BFS 或深度优先搜索 DFS）。根据上一步的评估分数，算法会决定是继续深入、放弃（剪枝）还是回溯分支。

这个过程是一个由您的代码主导、LLM 参与的循环：`代码(规划) -> LLM(生成) -> LLM(评估) -> 代码(决策) -> ...`，因此无法在标准聊天界面中完成。

### 简单类比：解数独

-   **CoT**: "我先把第一行的数字填上，然后第二行..." (一条路走到底)
-   **ToT**: "在左上角的格子里，我可以填 1、4 或 9。让我分别评估一下：
    *   如果填 1，第一行看起来没问题。
    *   如果填 4，会和第三列的 4 冲突，这是个死胡同，放弃这个分支。
    *   如果填 9，看起来也还行。
    *   好了，‘填 1’ 的方案看起来最有希望，我先沿着这条路往下走一步看看。"

## ReAct (Reason and Act) Prompting

ReAct 是一个将语言模型从单纯的“思考者”转变为既能“思考”又能“行动”的智能代理（Agent）的强大范式。它通过一个交错的 **思考 -> 行动 -> 观察** 循环来解决问题，将模型的推理能力与使用外部工具（如搜索引擎、计算器、API）的能力结合起来。

这个框架**必须通过编程和 API 调用来实现**，因为它需要一个外部的代理执行器（Agent Executor）来解析模型生成的“行动”并实际执行它们。

### ReAct 的核心循环

1.  **思考 (Thought)**: LLM 分析当前问题和已有信息，制定一个计划，并决定下一步需要什么信息以及如何通过工具获取它。
2.  **行动 (Action)**: LLM 生成一个对外部工具的调用指令，这个指令需要是机器可解析的（例如，`search("Apple Inc. founders")`）。
3.  **观察 (Observation)**: 代理执行器执行这个指令，并将从工具返回的结果作为“观察”反馈给 LLM。

LLM 会基于新的“观察”结果，进入下一轮的“思考”，如此循环，直到它认为自己已经收集到足够的信息来给出最终答案。

### 示例：ReAct 如何回答问题

**问题**: "苹果公司（Apple Inc.）的创始人是谁？他们中年龄最大的是谁，活了多久？"

**代理执行流程**:

1.  **第 1 轮**
    *   **思考**: 我需要先找出苹果公司的创始人。我可以使用搜索工具。
    *   **行动**: `search("founders of Apple Inc.")`

2.  **程序执行与观察**
    *   *(代理执行器调用搜索引擎)*
    *   **观察**: "苹果公司的创始人是史蒂夫·乔布斯（Steve Jobs）、斯蒂夫·沃兹尼亚克（Steve Wozniak）和罗纳德·韦恩（Ronald Wayne）。"

3.  **第 2 轮**
    *   **思考**: 我知道了三位创始人。接下来我需要找出他们每个人的出生和去世日期，来计算年龄和寿命。我会先从史蒂夫·乔布斯开始。
    *   **行动**: `search("Steve Jobs birth and death dates")`

4.  **程序执行与观察**
    *   *(代理执行器调用搜索引擎)*
    *   **观察**: "史蒂夫·乔布斯生于 1955 年 2 月 24 日，卒于 2011 年 10 月 5 日。"

5.  **...后续循环...**
    *   模型会继续查询其他创始人的信息。

6.  **最后一步**
    *   **思考**: 我已经收集了所有创始人的信息。通过比较，罗纳德·韦恩年龄最大。我现在拥有了回答最终问题所需的所有信息。
    *   **最终答案**: 苹果公司的创始人是史蒂夫·乔布斯、斯蒂夫·沃兹尼亚克和罗纳德·韦恩。其中年龄最大的是罗纳德·韦恩。史蒂夫·乔布斯活了56岁。

ReAct 框架通过赋予 LLM “使用工具”的能力，极大地扩展了其应用范围，使其能够解决更复杂的现实世界问题。

## System Prompting

System Prompt（系统提示）是为大型语言模型（LLM）设定的一个**全局指令、角色设定或行为“宪法”**。它在整个对话开始之前被设置，用于定义模型的个性、遵循的规则、回答的风格以及它的核心任务。

它和用户在每一轮对话中输入的“用户提示 (User Prompt)”不同：用户提示是“问什么”，而系统提示是“**你是谁**”以及“**你该如何回答**”。

> **核心定义:**
> "System prompting sets the overall context, purpose, and operational guidelines for LLMs. It defines the model's role, behavioral constraints, output format requirements, and safety guardrails. System prompts provide foundational parameters that influence all subsequent interactions, ensuring consistent, controlled, and structured AI responses throughout the session."

### 主要作用

1.  **定义角色和个性 (Define Role and Persona)**: 让模型扮演特定角色，如“资深软件架构师”或“乐于助人的教学助手”。
2.  **设定规则和约束 (Set Rules and Constraints)**: 为模型的行为划定边界，如“回答必须使用中文”、“答案不能超过200字”。
3.  **规定输出格式 (Specify Output Format)**: 强制要求特定的输出结构，如 Markdown、JSON 等，便于程序解析。
4.  **提供全局上下文 (Provide Global Context)**: 给予模型在整个对话中都应记住的背景信息，如“当前用户是初学者”。

### 示例

*   **System Prompt**:
    > "你是一个名为 'CodeHelper' 的 AI 编程助手。你的职责是帮助用户解决编程问题。请遵循以下规则：
    > 1. 总是先提供完整、可运行的代码示例。
    > 2. 在代码之后，提供对代码的详细、分步解释。
    > 3. 保持友好和鼓励的语气。"

*   **User Prompt**:
    > "如何用 Python 反转一个字符串？"

*   **理想的 LLM 回答**: (模型会严格按照 System Prompt 的设定来生成一个包含代码、解释且语气友好的回答。)

在实际的 API 调用中，System Prompt 通常是作为对话历史列表中的第一条消息，并被赋予一个特殊的 `system` 角色。

## Role Prompting (角色提示)

Role Prompting 是一种为 LLM 分配一个具体的角色、身份或职业的策略，以引导它生成符合该角色专业知识、个性和沟通风格的回答。

**关键关系：** Role Prompting 并不是一个独立于 System Prompting 的技术，而是 **System Prompting 最核心、最常见的应用之一**。在实际操作中，我们正是通过 System Prompt 这个机制，来具体实现 Role Prompting 这个策略。

### 为什么 Role Prompting 有效？

1.  **激活“专家网络”**: 赋予角色（如“资深物理学家”）会激活模型内部与该角色相关的知识、术语和思维模式，相当于在庞大的神经网络中点亮了相关的“专家”区域。
2.  **约束输出空间**: 设定角色（如“五岁小孩的睡前故事大王”）会极大地约束模型的回答范围，使其输出更聚焦、更可预测。
3.  **设定口吻和视角**: 这是控制模型沟通风格（如正式、风趣、批判等）最简单有效的方法。

### 实际应用方式举例

以下是将 Role Prompting 应用于不同场景的例子，通常将这些角色描述写入 **System Prompt** 中：

**1. 内容创作**
*   **角色设定**: "你是一位风趣幽默、充满激情的旅行博主，擅长用生动的语言描绘旅途中的见闻和感受。"
*   **任务**: "写一篇关于京都三日游的攻略。"
*   **结果**: 得到一篇充满个人色彩的游记，而非干巴巴的条目。

**2. 教育辅导**
*   **角色设定**: "你是一位严格但公平的数学老师，你从不直接给出答案，而是通过提问和引导，帮助学生自己找到解题思路。"
*   **任务**: "这道微积分题我不会做..."
*   **结果**: 获得引导式的教学体验，而非直接的答案。

**3. 编程辅助**
*   **角色设定**: "你是一位经验丰富的代码审查（Code Review）专家。你的目标是找出潜在的 bug、提出性能改进建议，并确保代码风格符合规范。请将你的建议以列表形式输出。"
*   **任务**: (用户粘贴一段代码)
*   **结果**: 获得结构化的、可执行的专业代码审查意见。

**4. 模拟面试**
*   **角色设定**: "你是一家顶级科技公司的招聘经理，现在你将对我进行一场关于前端工程师岗位的技术面试。"
*   **任务**: "你好，面试可以开始了。"
*   **结果**: 开始一场逼真的、交互式的模拟面试。

## Few-Shot Prompting (少样本提示)

Few-Shot Prompting 是一种通过在 Prompt 中提供几个完整的示例（Shots）来“教”会模型如何执行特定任务的技术。通过模仿学习，模型能更精确地理解用户的意图、格式和逻辑。

根据示例数量，通常分为：
-   **Zero-Shot (零样本)**: 不提供任何示例，直接提问。
-   **One-Shot (单样本)**: 提供一个示例。
-   **Few-Shot (少样本)**: 提供两个或更多示例。

### 为什么 Few-Shot 很重要？

1.  **提升可靠性**: 示例能清晰地展示期望的输出，减少模型对指令的“误解”。
2.  **教授复杂模式**: 对于 Chain-of-Thought (CoT) 或特定的输出格式（如JSON），提供示例是最高效的教学方式。

### 示例：使用 Few-Shot 进行情感分类

> **示例1**: "我爱这部电影，演技太棒了。"
> **情感**: 积极
>
> **示例2**: "完全是浪费时间，我看到一半就睡着了。"
> **情感**: 消极
>
> **示例3**: "这部电影还行，但情节有点慢。"
> **情感**: 中性
>
> ---
>
> **问题**: "绝对的杰作！今年看过最好的电影！"
> **情感**:

## Contextual Prompting (上下文提示)

Contextual Prompting 是一种为 LLM 提供与当前任务相关的、具体的背景信息或情景细节，以帮助其理解细微差别并生成更贴切回答的技术。

### 与其他 Prompt 的区别

-   **System/Role Prompt**: 是**静态的、全局的**，定义了模型在整个对话中的角色和行为准则。
-   **Contextual Prompt**: 是**动态的、局部的**，为当前这一个特定任务提供“即时”的背景信息。

### 与高级框架的关系

Contextual Prompting 是 RAG 和 ReAct 等高级框架自动化实现的核心思想。
-   **RAG (检索增强生成)**: 其核心就是程序化地检索信息，并将这些信息作为“Context”注入到 Prompt 中，自动构建一个完美的 Contextual Prompt。
-   **ReAct**: 在其 `思考 -> 行动 -> 观察` 的循环中，每一步的“观察”（工具返回的结果）都动态地成为下一步“思考”的即时上下文。

### 手动应用示例

**1. 会议纪要总结**
-   **不佳的 Prompt**: "总结一下这份会议纪要。"
-   **使用 Contextual Prompt**:
    > **Context**: 我是项目经理，错过了这次会议。我最关心的是会议上做出了哪些**最终决定**，以及分配给**前端团队**的**后续任务**有哪些。
    > **Task**: 基于以上关注点，总结这份会议纪要。

**2. 代码重构**
-   **不佳的 Prompt**: "帮我重构这段代码。"
-   **使用 Contextual Prompt**:
    > **Context**: 这段 Python 代码运行在一个对性能要求极高的数据处理管道中，目前的实现太慢了，成为了瓶颈。
    > **Task**: 请重构这段代码，核心目标是提升其运行性能。

## Automatic Prompt Engineering (APE)

Automatic Prompt Engineering (APE) 是一种利用大型语言模型（LLM）来自动生成、测试和优化 Prompt 的高级技术。这标志着我们从“如何为 AI 写好指令”，迈向了“**如何让 AI 自己找到最好的指令**”。

> **核心定义:**
> APE uses LLMs to generate and optimize prompts automatically, reducing human effort while enhancing model performance. The process involves prompting a model to create multiple prompt variants, evaluating them against a specific metric, and then selecting the highest-scoring candidate. This iterative approach helps discover effective prompts that humans might not consider.

### APE 的详细工作流程

1.  **定义“元任务” (Define the "Meta-Task")**:
    *   首先，由人类工程师定义一个高层次的目标，例如：“我需要找到一个最高效的 Prompt，让 LLM 能将复杂的医疗报告，准确地总结成普通人能看懂的几句话。”

2.  **候选提示的生成 (Candidate Prompt Generation)**:
    *   使用一个强大的“生成模型” LLM，并给予它一个“元提示”（Meta-Prompt），要求它围绕核心任务生成大量风格各异的候选 Prompt。

3.  **候选提示的评估 (Candidate Prompt Evaluation)**:
    *   这是最关键的一步。准备一个包含“输入 -> 理想输出”对的评估数据集（例如，100份医疗报告及其对应的“黄金标准”人工总结）。
    *   程序化地让**每一个**候选 Prompt 处理整个评估数据集。
    *   使用一个**评分函数**（Scoring Function）来比较 AI 的输出和“黄金标准”输出的质量。这个函数可以是一个经典的算法（如 ROUGE），也可以是另一个更强大的“评估模型” LLM。
    *   每个候选 Prompt 最终会得到一个量化的平均分。

4.  **选择与迭代 (Selection & Iteration)**:
    *   选择在评估中平均分最高的 Prompt 作为当前的最优解。
    *   （可选）将表现最好的几个 Prompt 重新作为种子，送回第二步进行迭代，生成更优的变体。

### APE 的重要性

-   **超越人类直觉**: APE 经常能发现一些人类工程师意想不到的、但效果奇佳的 Prompt。
-   **系统化与可复现**: 它将 Prompt Engineering 从一门“艺术”变成了一套有数据支撑的“科学”。
-   **自动化与效率**: 为生产环境中的大型应用提供了一套自动化寻找并验证最优 Prompt 的流程，是构建可靠 AI 产品的关键。
****